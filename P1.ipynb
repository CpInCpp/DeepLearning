{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "for k in range(12):\n",
    "    plt.subplot(3,4,k+1)\n",
    "    plt.imshow(X_train[k], cmap='Greys')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bf8b608dd8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_valid[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000,784).astype('float32')\n",
    "X_valid = X_valid.reshape(10000,784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_valid /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.32941177, 0.7254902 , 0.62352943,\n",
       "       0.5921569 , 0.23529412, 0.14117648, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.87058824, 0.99607843, 0.99607843, 0.99607843, 0.99607843,\n",
       "       0.94509804, 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "       0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.6666667 ,\n",
       "       0.20392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.2627451 , 0.44705883,\n",
       "       0.28235295, 0.44705883, 0.6392157 , 0.8901961 , 0.99607843,\n",
       "       0.88235295, 0.99607843, 0.99607843, 0.99607843, 0.98039216,\n",
       "       0.8980392 , 0.99607843, 0.99607843, 0.54901963, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "       0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "       0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.3254902 , 0.99215686, 0.81960785, 0.07058824,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.9137255 ,\n",
       "       1.        , 0.3254902 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.5058824 , 0.99607843, 0.93333334, 0.17254902,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.23137255, 0.9764706 ,\n",
       "       0.99607843, 0.24313726, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03529412, 0.8039216 ,\n",
       "       0.972549  , 0.22745098, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.49411765, 0.99607843, 0.7137255 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.29411766, 0.9843137 ,\n",
       "       0.9411765 , 0.22352941, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.07450981, 0.8666667 , 0.99607843, 0.6509804 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "       0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.14901961, 0.99607843, 0.99607843, 0.3019608 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.12156863, 0.8784314 , 0.99607843,\n",
       "       0.4509804 , 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.52156866, 0.99607843, 0.99607843, 0.20392157, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.23921569, 0.9490196 , 0.99607843,\n",
       "       0.99607843, 0.20392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.4745098 , 0.99607843,\n",
       "       0.8117647 , 0.07058824, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_valid = to_categorical(y_valid, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50176"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64*784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50240"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(64*784) + 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10*64) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\nanot\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.0926 - acc: 0.0885 - val_loss: 0.0921 - val_acc: 0.0946\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.0918 - acc: 0.0994 - val_loss: 0.0915 - val_acc: 0.1029\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.0912 - acc: 0.1065 - val_loss: 0.0909 - val_acc: 0.1127\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.0907 - acc: 0.1171 - val_loss: 0.0905 - val_acc: 0.1260\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0903 - acc: 0.1375 - val_loss: 0.0901 - val_acc: 0.1494\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.0899 - acc: 0.1630 - val_loss: 0.0897 - val_acc: 0.1757\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0896 - acc: 0.1880 - val_loss: 0.0894 - val_acc: 0.2033\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0893 - acc: 0.2115 - val_loss: 0.0891 - val_acc: 0.2234\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0890 - acc: 0.2343 - val_loss: 0.0888 - val_acc: 0.2422\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0887 - acc: 0.2557 - val_loss: 0.0885 - val_acc: 0.2699\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0884 - acc: 0.2878 - val_loss: 0.0882 - val_acc: 0.3091\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0881 - acc: 0.3247 - val_loss: 0.0879 - val_acc: 0.3397\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0879 - acc: 0.3487 - val_loss: 0.0877 - val_acc: 0.3591\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0876 - acc: 0.3641 - val_loss: 0.0874 - val_acc: 0.3699\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0873 - acc: 0.3742 - val_loss: 0.0871 - val_acc: 0.3788\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0871 - acc: 0.3806 - val_loss: 0.0869 - val_acc: 0.3827\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0868 - acc: 0.3842 - val_loss: 0.0866 - val_acc: 0.3868\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.0865 - acc: 0.386 - 2s 28us/sample - loss: 0.0865 - acc: 0.3865 - val_loss: 0.0863 - val_acc: 0.3874\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0862 - acc: 0.3887 - val_loss: 0.0860 - val_acc: 0.3901\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0860 - acc: 0.3904 - val_loss: 0.0857 - val_acc: 0.3920\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0857 - acc: 0.3925 - val_loss: 0.0854 - val_acc: 0.3931\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0854 - acc: 0.3937 - val_loss: 0.0851 - val_acc: 0.3961\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0851 - acc: 0.3972 - val_loss: 0.0848 - val_acc: 0.3971\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0847 - acc: 0.3986 - val_loss: 0.0845 - val_acc: 0.3996\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0844 - acc: 0.4019 - val_loss: 0.0842 - val_acc: 0.4017\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0841 - acc: 0.4032 - val_loss: 0.0838 - val_acc: 0.4047\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0838 - acc: 0.4050 - val_loss: 0.0835 - val_acc: 0.4075\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0834 - acc: 0.4079 - val_loss: 0.0831 - val_acc: 0.4110\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0831 - acc: 0.4108 - val_loss: 0.0828 - val_acc: 0.4143\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0827 - acc: 0.4132 - val_loss: 0.0824 - val_acc: 0.4161\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0823 - acc: 0.4160 - val_loss: 0.0820 - val_acc: 0.4202\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0820 - acc: 0.4194 - val_loss: 0.0816 - val_acc: 0.4231\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0816 - acc: 0.4218 - val_loss: 0.0812 - val_acc: 0.4266\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0812 - acc: 0.4255 - val_loss: 0.0808 - val_acc: 0.4310\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0808 - acc: 0.4297 - val_loss: 0.0804 - val_acc: 0.4348\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0804 - acc: 0.4347 - val_loss: 0.0800 - val_acc: 0.4397\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0799 - acc: 0.4381 - val_loss: 0.0796 - val_acc: 0.4428\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0795 - acc: 0.4424 - val_loss: 0.0791 - val_acc: 0.4466\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0791 - acc: 0.4468 - val_loss: 0.0787 - val_acc: 0.4504\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0786 - acc: 0.4508 - val_loss: 0.0783 - val_acc: 0.4562\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0782 - acc: 0.4552 - val_loss: 0.0778 - val_acc: 0.4616\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0777 - acc: 0.4607 - val_loss: 0.0773 - val_acc: 0.4676\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0773 - acc: 0.4666 - val_loss: 0.0769 - val_acc: 0.4735\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0768 - acc: 0.4711 - val_loss: 0.0764 - val_acc: 0.4796\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0763 - acc: 0.4773 - val_loss: 0.0759 - val_acc: 0.4847\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0759 - acc: 0.4834 - val_loss: 0.0754 - val_acc: 0.4916\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0754 - acc: 0.4897 - val_loss: 0.0749 - val_acc: 0.4968\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0749 - acc: 0.4955 - val_loss: 0.0744 - val_acc: 0.5037\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0744 - acc: 0.5016 - val_loss: 0.0740 - val_acc: 0.5098\n",
      "Epoch 50/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0739 - acc: 0.5076 - val_loss: 0.0735 - val_acc: 0.5147\n",
      "Epoch 51/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0734 - acc: 0.5130 - val_loss: 0.0730 - val_acc: 0.5206\n",
      "Epoch 52/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0729 - acc: 0.5195 - val_loss: 0.0724 - val_acc: 0.5264\n",
      "Epoch 53/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0724 - acc: 0.5251 - val_loss: 0.0719 - val_acc: 0.5320\n",
      "Epoch 54/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0719 - acc: 0.5308 - val_loss: 0.0714 - val_acc: 0.5387\n",
      "Epoch 55/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0714 - acc: 0.5361 - val_loss: 0.0709 - val_acc: 0.5442\n",
      "Epoch 56/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0709 - acc: 0.5411 - val_loss: 0.0704 - val_acc: 0.5498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0704 - acc: 0.5469 - val_loss: 0.0699 - val_acc: 0.5547\n",
      "Epoch 58/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0699 - acc: 0.5523 - val_loss: 0.0694 - val_acc: 0.5613\n",
      "Epoch 59/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0694 - acc: 0.5573 - val_loss: 0.0689 - val_acc: 0.5677\n",
      "Epoch 60/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0689 - acc: 0.5625 - val_loss: 0.0684 - val_acc: 0.5722\n",
      "Epoch 61/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0684 - acc: 0.5678 - val_loss: 0.0679 - val_acc: 0.5763\n",
      "Epoch 62/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0679 - acc: 0.5728 - val_loss: 0.0674 - val_acc: 0.5802\n",
      "Epoch 63/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0674 - acc: 0.5777 - val_loss: 0.0669 - val_acc: 0.5844\n",
      "Epoch 64/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0669 - acc: 0.5826 - val_loss: 0.0664 - val_acc: 0.5883\n",
      "Epoch 65/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0664 - acc: 0.5864 - val_loss: 0.0659 - val_acc: 0.5925\n",
      "Epoch 66/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0659 - acc: 0.5910 - val_loss: 0.0654 - val_acc: 0.5966\n",
      "Epoch 67/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0654 - acc: 0.5951 - val_loss: 0.0649 - val_acc: 0.6001\n",
      "Epoch 68/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0650 - acc: 0.5993 - val_loss: 0.0644 - val_acc: 0.6038\n",
      "Epoch 69/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0645 - acc: 0.6030 - val_loss: 0.0639 - val_acc: 0.6073\n",
      "Epoch 70/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0640 - acc: 0.6074 - val_loss: 0.0634 - val_acc: 0.6110\n",
      "Epoch 71/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0635 - acc: 0.6115 - val_loss: 0.0629 - val_acc: 0.6151\n",
      "Epoch 72/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0631 - acc: 0.6153 - val_loss: 0.0625 - val_acc: 0.6200\n",
      "Epoch 73/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0626 - acc: 0.6187 - val_loss: 0.0620 - val_acc: 0.6234\n",
      "Epoch 74/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0621 - acc: 0.6230 - val_loss: 0.0615 - val_acc: 0.6270\n",
      "Epoch 75/200\n",
      "60000/60000 [==============================] - 2s 27us/sample - loss: 0.0617 - acc: 0.6266 - val_loss: 0.0611 - val_acc: 0.6298\n",
      "Epoch 76/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0612 - acc: 0.6301 - val_loss: 0.0606 - val_acc: 0.6342\n",
      "Epoch 77/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0607 - acc: 0.6339 - val_loss: 0.0601 - val_acc: 0.6374\n",
      "Epoch 78/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0603 - acc: 0.6374 - val_loss: 0.0597 - val_acc: 0.6413\n",
      "Epoch 79/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0599 - acc: 0.6408 - val_loss: 0.0592 - val_acc: 0.6448\n",
      "Epoch 80/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0594 - acc: 0.6437 - val_loss: 0.0588 - val_acc: 0.6472\n",
      "Epoch 81/200\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0590 - acc: 0.6466 - val_loss: 0.0584 - val_acc: 0.6502\n",
      "Epoch 82/200\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0585 - acc: 0.6500 - val_loss: 0.0579 - val_acc: 0.6535\n",
      "Epoch 83/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0581 - acc: 0.6529 - val_loss: 0.0575 - val_acc: 0.6565\n",
      "Epoch 84/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0577 - acc: 0.6557 - val_loss: 0.0571 - val_acc: 0.6585\n",
      "Epoch 85/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0573 - acc: 0.6587 - val_loss: 0.0566 - val_acc: 0.6615\n",
      "Epoch 86/200\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0569 - acc: 0.6613 - val_loss: 0.0562 - val_acc: 0.6638\n",
      "Epoch 87/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0565 - acc: 0.6641 - val_loss: 0.0558 - val_acc: 0.6662\n",
      "Epoch 88/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0560 - acc: 0.6665 - val_loss: 0.0554 - val_acc: 0.6697\n",
      "Epoch 89/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0556 - acc: 0.6689 - val_loss: 0.0550 - val_acc: 0.6715\n",
      "Epoch 90/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0552 - acc: 0.6712 - val_loss: 0.0546 - val_acc: 0.6732\n",
      "Epoch 91/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0549 - acc: 0.6728 - val_loss: 0.0542 - val_acc: 0.6751\n",
      "Epoch 92/200\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0545 - acc: 0.6749 - val_loss: 0.0538 - val_acc: 0.6777\n",
      "Epoch 93/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0541 - acc: 0.6769 - val_loss: 0.0534 - val_acc: 0.6798\n",
      "Epoch 94/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0537 - acc: 0.6784 - val_loss: 0.0530 - val_acc: 0.6816\n",
      "Epoch 95/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0533 - acc: 0.6804 - val_loss: 0.0526 - val_acc: 0.6838\n",
      "Epoch 96/200\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0529 - acc: 0.6820 - val_loss: 0.0523 - val_acc: 0.6854\n",
      "Epoch 97/200\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0526 - acc: 0.6834 - val_loss: 0.0519 - val_acc: 0.6876\n",
      "Epoch 98/200\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0522 - acc: 0.6851 - val_loss: 0.0515 - val_acc: 0.6887\n",
      "Epoch 99/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0519 - acc: 0.6866 - val_loss: 0.0512 - val_acc: 0.6905\n",
      "Epoch 100/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0515 - acc: 0.6880 - val_loss: 0.0508 - val_acc: 0.6925\n",
      "Epoch 101/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0511 - acc: 0.6893 - val_loss: 0.0504 - val_acc: 0.6937\n",
      "Epoch 102/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0508 - acc: 0.6910 - val_loss: 0.0501 - val_acc: 0.6959\n",
      "Epoch 103/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0504 - acc: 0.6922 - val_loss: 0.0497 - val_acc: 0.6982\n",
      "Epoch 104/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0501 - acc: 0.6938 - val_loss: 0.0494 - val_acc: 0.6997\n",
      "Epoch 105/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0498 - acc: 0.6950 - val_loss: 0.0490 - val_acc: 0.7012\n",
      "Epoch 106/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0494 - acc: 0.6963 - val_loss: 0.0487 - val_acc: 0.7024\n",
      "Epoch 107/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0491 - acc: 0.6977 - val_loss: 0.0484 - val_acc: 0.7036\n",
      "Epoch 108/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0488 - acc: 0.6991 - val_loss: 0.0480 - val_acc: 0.7043\n",
      "Epoch 109/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0484 - acc: 0.7001 - val_loss: 0.0477 - val_acc: 0.7057\n",
      "Epoch 110/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0481 - acc: 0.7012 - val_loss: 0.0474 - val_acc: 0.7072\n",
      "Epoch 111/200\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.0478 - acc: 0.7023 - val_loss: 0.0471 - val_acc: 0.7081\n",
      "Epoch 112/200\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0475 - acc: 0.7034 - val_loss: 0.0467 - val_acc: 0.7093\n",
      "Epoch 113/200\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0472 - acc: 0.7046 - val_loss: 0.0464 - val_acc: 0.7113\n",
      "Epoch 114/200\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0469 - acc: 0.7061 - val_loss: 0.0461 - val_acc: 0.7127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0466 - acc: 0.7076 - val_loss: 0.0458 - val_acc: 0.7144\n",
      "Epoch 116/200\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0463 - acc: 0.7092 - val_loss: 0.0455 - val_acc: 0.7167\n",
      "Epoch 117/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0460 - acc: 0.7110 - val_loss: 0.0452 - val_acc: 0.7196\n",
      "Epoch 118/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0457 - acc: 0.7129 - val_loss: 0.0449 - val_acc: 0.7218\n",
      "Epoch 119/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0454 - acc: 0.7156 - val_loss: 0.0446 - val_acc: 0.7232\n",
      "Epoch 120/200\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0451 - acc: 0.7177 - val_loss: 0.0443 - val_acc: 0.7262\n",
      "Epoch 121/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0448 - acc: 0.7206 - val_loss: 0.0440 - val_acc: 0.7282\n",
      "Epoch 122/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0445 - acc: 0.7234 - val_loss: 0.0437 - val_acc: 0.7305\n",
      "Epoch 123/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0442 - acc: 0.7256 - val_loss: 0.0434 - val_acc: 0.7333\n",
      "Epoch 124/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0439 - acc: 0.7286 - val_loss: 0.0431 - val_acc: 0.7366\n",
      "Epoch 125/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0437 - acc: 0.7316 - val_loss: 0.0429 - val_acc: 0.7386\n",
      "Epoch 126/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0434 - acc: 0.7347 - val_loss: 0.0426 - val_acc: 0.7415\n",
      "Epoch 127/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0431 - acc: 0.7383 - val_loss: 0.0423 - val_acc: 0.7449\n",
      "Epoch 128/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0429 - acc: 0.7412 - val_loss: 0.0420 - val_acc: 0.7485\n",
      "Epoch 129/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0426 - acc: 0.7444 - val_loss: 0.0418 - val_acc: 0.7518\n",
      "Epoch 130/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0423 - acc: 0.7482 - val_loss: 0.0415 - val_acc: 0.7555\n",
      "Epoch 131/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0421 - acc: 0.7512 - val_loss: 0.0412 - val_acc: 0.7593\n",
      "Epoch 132/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0418 - acc: 0.7539 - val_loss: 0.0410 - val_acc: 0.7615\n",
      "Epoch 133/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0416 - acc: 0.7568 - val_loss: 0.0407 - val_acc: 0.7668\n",
      "Epoch 134/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0413 - acc: 0.7605 - val_loss: 0.0405 - val_acc: 0.7693\n",
      "Epoch 135/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0410 - acc: 0.7631 - val_loss: 0.0402 - val_acc: 0.7733\n",
      "Epoch 136/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0408 - acc: 0.7666 - val_loss: 0.0399 - val_acc: 0.7772\n",
      "Epoch 137/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0406 - acc: 0.7700 - val_loss: 0.0397 - val_acc: 0.7807\n",
      "Epoch 138/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0403 - acc: 0.7733 - val_loss: 0.0395 - val_acc: 0.7826\n",
      "Epoch 139/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0401 - acc: 0.7759 - val_loss: 0.0392 - val_acc: 0.7852\n",
      "Epoch 140/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0398 - acc: 0.7792 - val_loss: 0.0390 - val_acc: 0.7885\n",
      "Epoch 141/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0396 - acc: 0.7826 - val_loss: 0.0387 - val_acc: 0.7911\n",
      "Epoch 142/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0394 - acc: 0.7850 - val_loss: 0.0385 - val_acc: 0.7945\n",
      "Epoch 143/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0391 - acc: 0.7882 - val_loss: 0.0383 - val_acc: 0.7971\n",
      "Epoch 144/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0389 - acc: 0.7904 - val_loss: 0.0380 - val_acc: 0.7994\n",
      "Epoch 145/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0387 - acc: 0.7930 - val_loss: 0.0378 - val_acc: 0.8022\n",
      "Epoch 146/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0385 - acc: 0.7958 - val_loss: 0.0376 - val_acc: 0.8044\n",
      "Epoch 147/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0382 - acc: 0.7984 - val_loss: 0.0373 - val_acc: 0.8069\n",
      "Epoch 148/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0380 - acc: 0.8009 - val_loss: 0.0371 - val_acc: 0.8091\n",
      "Epoch 149/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0378 - acc: 0.8034 - val_loss: 0.0369 - val_acc: 0.8120\n",
      "Epoch 150/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0376 - acc: 0.8057 - val_loss: 0.0367 - val_acc: 0.8148\n",
      "Epoch 151/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0374 - acc: 0.8075 - val_loss: 0.0365 - val_acc: 0.8165\n",
      "Epoch 152/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0372 - acc: 0.8103 - val_loss: 0.0363 - val_acc: 0.8187\n",
      "Epoch 153/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0370 - acc: 0.8124 - val_loss: 0.0360 - val_acc: 0.8207\n",
      "Epoch 154/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0367 - acc: 0.8149 - val_loss: 0.0358 - val_acc: 0.8216\n",
      "Epoch 155/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0365 - acc: 0.8162 - val_loss: 0.0356 - val_acc: 0.8234\n",
      "Epoch 156/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0363 - acc: 0.8179 - val_loss: 0.0354 - val_acc: 0.8258\n",
      "Epoch 157/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0361 - acc: 0.8199 - val_loss: 0.0352 - val_acc: 0.8267\n",
      "Epoch 158/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0359 - acc: 0.8212 - val_loss: 0.0350 - val_acc: 0.8286\n",
      "Epoch 159/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0358 - acc: 0.8233 - val_loss: 0.0348 - val_acc: 0.8300\n",
      "Epoch 160/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0356 - acc: 0.8246 - val_loss: 0.0346 - val_acc: 0.8317\n",
      "Epoch 161/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0354 - acc: 0.8264 - val_loss: 0.0344 - val_acc: 0.8333\n",
      "Epoch 162/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0352 - acc: 0.8278 - val_loss: 0.0342 - val_acc: 0.8350\n",
      "Epoch 163/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0350 - acc: 0.8292 - val_loss: 0.0341 - val_acc: 0.8359\n",
      "Epoch 164/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0348 - acc: 0.8306 - val_loss: 0.0339 - val_acc: 0.8376\n",
      "Epoch 165/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0346 - acc: 0.8315 - val_loss: 0.0337 - val_acc: 0.8387\n",
      "Epoch 166/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0344 - acc: 0.8330 - val_loss: 0.0335 - val_acc: 0.8399\n",
      "Epoch 167/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0343 - acc: 0.8345 - val_loss: 0.0333 - val_acc: 0.8408\n",
      "Epoch 168/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0341 - acc: 0.8356 - val_loss: 0.0331 - val_acc: 0.8421\n",
      "Epoch 169/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0339 - acc: 0.8365 - val_loss: 0.0330 - val_acc: 0.8431\n",
      "Epoch 170/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0337 - acc: 0.8376 - val_loss: 0.0328 - val_acc: 0.8442\n",
      "Epoch 171/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0336 - acc: 0.8386 - val_loss: 0.0326 - val_acc: 0.8450\n",
      "Epoch 172/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0334 - acc: 0.8397 - val_loss: 0.0324 - val_acc: 0.8464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0332 - acc: 0.8406 - val_loss: 0.0323 - val_acc: 0.8475\n",
      "Epoch 174/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0331 - acc: 0.8413 - val_loss: 0.0321 - val_acc: 0.8483\n",
      "Epoch 175/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0329 - acc: 0.8425 - val_loss: 0.0319 - val_acc: 0.8487\n",
      "Epoch 176/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0327 - acc: 0.8435 - val_loss: 0.0318 - val_acc: 0.8492\n",
      "Epoch 177/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0326 - acc: 0.8445 - val_loss: 0.0316 - val_acc: 0.8502\n",
      "Epoch 178/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0324 - acc: 0.8452 - val_loss: 0.0315 - val_acc: 0.8506\n",
      "Epoch 179/200\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0323 - acc: 0.8460 - val_loss: 0.0313 - val_acc: 0.8511\n",
      "Epoch 180/200\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0321 - acc: 0.8467 - val_loss: 0.0312 - val_acc: 0.8516\n",
      "Epoch 181/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0320 - acc: 0.8472 - val_loss: 0.0310 - val_acc: 0.8527\n",
      "Epoch 182/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0318 - acc: 0.8478 - val_loss: 0.0308 - val_acc: 0.8539\n",
      "Epoch 183/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0317 - acc: 0.8484 - val_loss: 0.0307 - val_acc: 0.8544\n",
      "Epoch 184/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0315 - acc: 0.8488 - val_loss: 0.0305 - val_acc: 0.8550\n",
      "Epoch 185/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0314 - acc: 0.8497 - val_loss: 0.0304 - val_acc: 0.8558\n",
      "Epoch 186/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0312 - acc: 0.8503 - val_loss: 0.0303 - val_acc: 0.8564\n",
      "Epoch 187/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0311 - acc: 0.8508 - val_loss: 0.0301 - val_acc: 0.8568\n",
      "Epoch 188/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0309 - acc: 0.8511 - val_loss: 0.0300 - val_acc: 0.8570\n",
      "Epoch 189/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0308 - acc: 0.8517 - val_loss: 0.0298 - val_acc: 0.8578\n",
      "Epoch 190/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0307 - acc: 0.8520 - val_loss: 0.0297 - val_acc: 0.8582\n",
      "Epoch 191/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0305 - acc: 0.8525 - val_loss: 0.0296 - val_acc: 0.8589\n",
      "Epoch 192/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0304 - acc: 0.8528 - val_loss: 0.0294 - val_acc: 0.8594\n",
      "Epoch 193/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0303 - acc: 0.8533 - val_loss: 0.0293 - val_acc: 0.8599\n",
      "Epoch 194/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0301 - acc: 0.8537 - val_loss: 0.0292 - val_acc: 0.8603\n",
      "Epoch 195/200\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0300 - acc: 0.8544 - val_loss: 0.0290 - val_acc: 0.8610\n",
      "Epoch 196/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0299 - acc: 0.8548 - val_loss: 0.0289 - val_acc: 0.8616\n",
      "Epoch 197/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0298 - acc: 0.8554 - val_loss: 0.0288 - val_acc: 0.8620\n",
      "Epoch 198/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0296 - acc: 0.8558 - val_loss: 0.0286 - val_acc: 0.8627\n",
      "Epoch 199/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0295 - acc: 0.8562 - val_loss: 0.0285 - val_acc: 0.8633\n",
      "Epoch 200/200\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.0294 - acc: 0.8565 - val_loss: 0.0284 - val_acc: 0.8635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bf87f9b160>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, epochs=200, verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0284 - acc: 0.8635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02839024303853512, 0.8635]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_0 = X_valid[0].reshape(1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00312072, 0.00173495, 0.00435913, 0.00807031, 0.00709541,\n",
       "        0.00826222, 0.00119123, 0.91885656, 0.00374568, 0.04356375]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(valid_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(valid_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
